# 方策の評価とその推定

## 推定量

データ$\mathcal{D} = \{x_i\}_{i=1}^n$は同時分布$p(\mathcal{D}) = p(x_1, \dots, x_n) = \prod_{i=1}^n p(x_i)$から独立に生成されたものとする.
このデータに基づき, 推定目標$V$を統計的に近似する**推定量(estimator)**$\hat{V}(\mathcal{D})$を構築する.
このとき推定量を評価するために, 推定量の統計的性質を分析することが重要.

## 推定量の性質

今回は推定量の統計的性質として, バイアスとバリアンスを考える.
推定量$\hat{V}$のバイアスとは, 推定目標と推定量の期待値の差であり, 次のように定義される.
$$\rm{Bias}[\hat{V}(\mathcal{D})] = |V - \mathbb{E}_{p(\mathcal{D})}[\hat{V}(\mathcal{D})]|$$
推定量$\hat{V}$のバリアンスとは, 推定量のばらつきの大きさであり, 次のように定義される.
$$\rm{Var}[\hat{V}(\mathcal{D})] = \mathbb{E}_{p(\mathcal{D})}[(\hat{V}(\mathcal{D}) - \mathbb{E}_{p(\mathcal{D})}[\hat{V}(\mathcal{D})])^2]$$

:::note info
平均二乗誤差(Mean Squared Error: MSE)はバイアスとバリアンスの和で表される.

```math
\rm{MSE}[\hat{V}(\mathcal{D})] = \rm{Bias}[\hat{V}(\mathcal{D})]^2 + \rm{Var}[\hat{V}(\mathcal{D})]
```

推定量の性質を分析する際は, その推定量のバイアスとバリアンスを計算し, 比較検討することが定石.

## 意思決定における方策と評価

### 機械学習と意思決定

機械学習は, データに基づいた性格の予測を可能にする技術としていたるところで用いられる.
しかし, マーケティングなどの実務においては, 機械学習による予測値をそのまま用いるのではなく, 予測値に基づいて何らかの意思決定をおこなっていることが多い.
この問題を定式化する.まず特徴量(feature, context)ベクトルを$x\in \mathcal{X}$, 離散的な行動(action)を$a\in \mathcal{A}$, 行動の結果として観測される報酬(reward)を$r\in \mathbb{R}$とする.
意思決定において, 方策(policy)とは, 特徴量$x$を観測したときに, どのような行動$a$を選択するかを決定する関数である.この意思決定方策$\pi: \mathcal{X} \to \Delta(\mathcal{A})$を行動空間$\mathcal{A}$上の条件付き確率分布として導入する.

1. 特徴量を観測する　$x_i \sim p(x)$
2. 方策が行動を選択する $a_i \sim \pi(a|x_i)$
3. 報酬を観測する $r_i \sim p(r|x_i, a_i)$

| 応用例       | 特徴量$x$            | 方策$a$        | 報酬$r$            |
| ------------ | -------------------- | -------------- | ------------------ |
| 映画推薦     | ユーザの映画視聴履歴 | 映画           | クリック・視聴時間 |
| クーポン配布 | ユーザの購買履歴     | クーポンの種類 | 購買有無・売上     |
| 解約抑止     | 顧客のデータ         | DM送付の有無   | 解約有無           |

::: note info
定義1.1 意思決定方策$\pi$の性能(policy value)は, 次のように定義される.
$$V(\pi) = \mathbb{E}_{p(x)\pi(a|x)p(r | x, a)}[r|x, a] = \mathbb{E}_{p(x)\pi(a|x)} [q(x, a)]$$

なお, $q(x, a) := \mathbb{E}_{p(r|x, a)}[r]$は特徴量$x$と方策$a$で条件づけたときの報酬$r$の期待値であり, 期待報酬関数と呼ぶ.

## 方策性能の推定量とその性質

### Direct Method
