{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import time\n",
    "import mlflow.system_metrics\n",
    "from logging import getLogger, StreamHandler, INFO\n",
    "import xgboost\n",
    "import shap\n",
    "import mlflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\n",
    "    \"http://ec2-44-217-145-52.compute-1.amazonaws.com:5000\"\n",
    ")\n",
    "experiment = mlflow.set_experiment(\"test\")\n",
    "mlflow.system_metrics.enable_system_metrics_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/27 08:24:42 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n"
     ]
    }
   ],
   "source": [
    "run = mlflow.start_run(experiment_id=experiment.experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_metrics({\"tlearner_auuc\": 0.1, \"tlearner_uplift\": 0.01})\n",
    "time.sleep(10)\n",
    "mlflow.log_metrics({\"slearner_auuc\": 0.2, \"tlearner_uplift\": 0.02})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load UCI Adult Data Set; segment it into training and test sets\n",
    "X, y = shap.datasets.adult()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "# train XGBoost model\n",
    "model = xgboost.XGBClassifier().fit(X_train, y_train)\n",
    "\n",
    "# construct an evaluation dataset from the test set\n",
    "eval_data = X_test\n",
    "eval_data[\"target\"] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load UCI Adult Data Set; segment it into training and test sets\n",
    "X, y = shap.datasets.adult()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "# train XGBoost model\n",
    "model = xgboost.XGBClassifier().fit(X_train, y_train)\n",
    "\n",
    "# construct an evaluation dataset from the test set\n",
    "eval_data = X_test\n",
    "eval_data[\"target\"] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/27 08:25:21 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 13.55it/s]\n",
      "/workspace/.venv/lib/python3.11/site-packages/mlflow/types/utils.py:407: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "2024/10/27 08:25:33 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "2024/10/27 08:25:33 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1, negative label is 0.\n",
      "2024/10/27 08:25:33 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2024/10/27 08:25:35 INFO mlflow.models.evaluation.default_evaluator: Shap explainer TreeExplainer is used.\n",
      "/workspace/.venv/lib/python3.11/site-packages/mlflow/shap/__init__.py:436: UserWarning: Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "  warnings.warn(\n",
      "2024/10/27 08:25:41 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'TreeEnsemble' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_info = mlflow.sklearn.log_model(model, \"model\")\n",
    "result = mlflow.evaluate(\n",
    "    model_info.model_uri,\n",
    "    eval_data,\n",
    "    targets=\"target\",\n",
    "    model_type=\"classifier\",\n",
    "    evaluators=[\"default\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_custom_metric_fn(eval_df, builtin_metrics, artifacts_dir):\n",
    "    \"\"\"\n",
    "    This example custom metric function creates a metric based on the ``prediction`` and\n",
    "    ``target`` columns in ``eval_df`` and a metric derived from existing metrics in\n",
    "    ``builtin_metrics``. It also generates and saves a scatter plot to ``artifacts_dir`` that\n",
    "    visualizes the relationship between the predictions and targets for the given model to a\n",
    "    file as an image artifact.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"squared_diff_plus_one\": np.sum(\n",
    "            np.abs(eval_df[\"prediction\"] - eval_df[\"target\"] + 1) ** 2\n",
    "        ),\n",
    "        \"sum_on_label_divided_by_two\": builtin_metrics[\"sum_on_label\"] / 2,\n",
    "    }\n",
    "    plt.scatter(eval_df[\"prediction\"], eval_df[\"target\"])\n",
    "    plt.xlabel(\"Targets\")\n",
    "    plt.ylabel(\"Predictions\")\n",
    "    plt.title(\"Targets vs. Predictions\")\n",
    "    plot_path = os.path.join(artifacts_dir, \"example_scatter_plot.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    artifacts = {\"example_scatter_plot_artifact\": plot_path}\n",
    "    return metrics, artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_table(eval_data, artifact_file=\"eval_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/27 08:25:44 INFO mlflow.tracking._tracking_service.client: üèÉ View run zealous-squid-821 at: http://ec2-44-217-145-52.compute-1.amazonaws.com:5000/#/experiments/1/runs/cd926aa703cd49fbbd291273f85d9ccd.\n",
      "2024/10/27 08:25:44 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://ec2-44-217-145-52.compute-1.amazonaws.com:5000/#/experiments/1.\n",
      "2024/10/27 08:25:44 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2024/10/27 08:25:44 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
